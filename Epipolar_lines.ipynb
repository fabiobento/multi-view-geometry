{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Epipolar lines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiobento/multi-view-geometry/blob/master/Epipolar_lines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybp3j5-bY_r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install opencv-python==3.4.2.16\n",
        "\n",
        "!pip install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiAxtVgMCzmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the necessary packages\n",
        "import cv2\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        " \n",
        "def url_to_image(url, readFlag=cv2.IMREAD_COLOR):\n",
        "    # download the image, convert it to a NumPy array, and then read\n",
        "    # it into OpenCV format\n",
        "    resp = urlopen(url)\n",
        "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "    image = cv2.imdecode(image, readFlag)\n",
        "\n",
        "    # return the image\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_lines(img_left, img_right, lines, pts_left, pts_right):\n",
        "    h,w = img_left.shape[:-1]\n",
        "    img_left = cv2.cvtColor(img_left, cv2.COLOR_RGB2BGR)\n",
        "    img_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    for line, pt_left, pt_right in zip(lines, pts_left, pts_right):\n",
        "        x_start,y_start = map(int, [0, -line[2]/line[1] ])\n",
        "        x_end,y_end = map(int, [w, -(line[2]+line[0]*w)/line[1] ])\n",
        "        color = tuple(np.random.randint(0,255,2).tolist())\n",
        "        cv2.line(img_left, (x_start,y_start), (x_end,y_end), color,1)\n",
        "        cv2.circle(img_left, tuple(pt_left), 5, color, -1)\n",
        "        cv2.circle(img_right, tuple(pt_right), 5, color, -1)\n",
        "\n",
        "    return img_left, img_right\n",
        "\n",
        "def get_descriptors(gray_image, feature_type):\n",
        "    if feature_type == 'surf':\n",
        "        feature_extractor = cv2.xfeatures2d.SURF_create()\n",
        "        #feature_extractor = cv2.SURF()\n",
        "    elif feature_type == 'sift':\n",
        "        feature_extractor = cv2.xfeatures2d.SIFT_create()\n",
        "        #        feature_extractor = cv2.SIFT()\n",
        "    else:\n",
        "        raise TypeError(\"Invalid feature type; should be either 'surf' or 'sift'\")\n",
        "\n",
        "    keypoints, descriptors = feature_extractor.detectAndCompute(gray_image,None)\n",
        "    #feature_extractor.detectAndCompute(gray_image, None)\n",
        "    return keypoints, descriptors\n",
        "\n",
        "\n",
        "left_image = \"https://raw.githubusercontent.com/fabiobento/multi-view-geometry/master/data/stereo/comicsStarWars01.jpg?token=AFCHXJIZESHUCIXYXQSS7VK42QXCY\"  \n",
        "right_image = \"https://raw.githubusercontent.com/fabiobento/multi-view-geometry/master/data/stereo/comicsStarWars02.jpg?token=AFCHXJPRSHOCKESRVAFK74C42QXMO\"\n",
        "img_left = url_to_image(left_image)\n",
        "img_right = url_to_image(right_image)\n",
        "\n",
        "feature_type = 'sift'\n",
        "if feature_type not in ['sift', 'surf']:\n",
        "    raise TypeError(\"Invalid feature type; has to be either 'sift' or 'surf'\")\n",
        "\n",
        "scaling_factor = 0.5\n",
        "img_left = cv2.resize(img_left, None, fx=scaling_factor,fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
        "img_right = cv2.resize(img_right, None, fx=scaling_factor,fy=scaling_factor, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "kps_left, des_left = get_descriptors(img_left, feature_type)\n",
        "kps_right, des_right = get_descriptors(img_right, feature_type)\n",
        "\n",
        "# FLANN parameters\n",
        "FLANN_INDEX_KDTREE = 0\n",
        "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
        "search_params = dict(checks=50)\n",
        "\n",
        "# Get the matches based on the descriptors\n",
        "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "matches = flann.knnMatch(des_left, des_right, k=2)\n",
        "\n",
        "pts_left_image = []\n",
        "pts_right_image = []\n",
        "\n",
        "# ratio test to retain only the good matches\n",
        "for i,(m,n) in enumerate(matches):\n",
        "    if m.distance < 0.7*n.distance:\n",
        "        pts_left_image.append(kps_left[m.queryIdx].pt)\n",
        "        pts_right_image.append(kps_right[m.trainIdx].pt)\n",
        "\n",
        "pts_left_image = np.float32(pts_left_image)\n",
        "pts_right_image = np.float32(pts_right_image)\n",
        "F, mask = cv2.findFundamentalMat(pts_left_image, pts_right_image, cv2.FM_LMEDS)\n",
        "\n",
        "# Selecting only the inliers\n",
        "pts_left_image = pts_left_image[mask.ravel()==1]\n",
        "pts_right_image = pts_right_image[mask.ravel()==1]\n",
        "\n",
        "# Drawing the lines on left image and the corresponding feature points on the right image\n",
        "lines1 = cv2.computeCorrespondEpilines (pts_right_image.reshape(-1,1,2), 2, F)\n",
        "lines1 = lines1.reshape(-1,3)\n",
        "img_left_lines, img_right_pts = draw_lines(img_left, img_right, lines1, pts_left_image, pts_right_image)\n",
        "\n",
        "# Drawing the lines on right image and the corresponding feature points on the left image\n",
        "lines2 = cv2.computeCorrespondEpilines (pts_left_image.reshape(-1,1,2), 1,F)\n",
        "lines2 = lines2.reshape(-1,3)\n",
        "img_right_lines, img_left_pts = draw_lines(img_right, img_left, lines2, pts_right_image, pts_left_image)\n",
        "\n",
        "print('Epi lines on left image')\n",
        "cv2_imshow(img_left_lines)\n",
        "print('\\nFeature points on right image')\n",
        "cv2_imshow(img_right_pts)\n",
        "print('\\nEpi lines on right image')\n",
        "cv2_imshow(img_right_lines)\n",
        "print('\\nFeature points on left image')\n",
        "cv2_imshow(img_left_pts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}